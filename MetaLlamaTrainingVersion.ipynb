{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716975c1-4f4f-4908-a478-b2899f1e1943",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show transformers torch llama-recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb80e6df-a1e5-41ab-958b-bd6ca3cc3a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login('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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc99d3d-6018-41c3-9d31-7e7f4a07386f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LlamaForCausalLM, AutoTokenizer\n",
    "from llama_recipes.configs import train_config as TRAIN_CONFIG\n",
    "\n",
    "train_config = TRAIN_CONFIG()\n",
    "train_config.model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "train_config.num_epochs = 1\n",
    "train_config.run_validation = False\n",
    "train_config.gradient_accumulation_steps = 4\n",
    "train_config.batch_size_training = 1\n",
    "train_config.lr = 3e-4\n",
    "train_config.use_fast_kernels = True\n",
    "train_config.use_fp16 = True\n",
    "train_config.context_length = 1024 if torch.cuda.get_device_properties(0).total_memory < 16e9 else 2048 # T4 16GB or A10 24GB\n",
    "train_config.batching_strategy = \"packing\"\n",
    "train_config.output_dir = \"meta-llama-samsum\"\n",
    "train_config.use_peft = True\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    ")\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "            train_config.model_name,\n",
    "            device_map=\"auto\",\n",
    "            quantization_config=config,\n",
    "            use_cache=False,\n",
    "            attn_implementation=\"sdpa\" if train_config.use_fast_kernels else None,\n",
    "            torch_dtype=torch.float16,\n",
    "        )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(train_config.model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1020a818-7672-45be-ba9f-a32da465c7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prompt = \"\"\"\n",
    "Summarize this dialog:\n",
    "A: Hi Tom, are you busy tomorrow’s afternoon?\n",
    "B: I’m pretty sure I am. What’s up?\n",
    "A: Can you go with me to the animal shelter?.\n",
    "B: What do you want to do?\n",
    "A: I want to get a puppy for my son.\n",
    "B: That will make him so happy.\n",
    "A: Yeah, we’ve discussed it many times. I think he’s ready now.\n",
    "B: That’s good. Raising a dog is a tough issue. Like having a baby ;-) \n",
    "A: I'll get him one of those little dogs.\n",
    "B: One that won't grow up too big;-)\n",
    "A: And eat too much;-))\n",
    "B: Do you know which one he would like?\n",
    "A: Oh, yes, I took him there last Monday. He showed me one that he really liked.\n",
    "B: I bet you had to drag him away.\n",
    "A: He wanted to take it home right away ;-).\n",
    "B: I wonder what he'll name it.\n",
    "A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))\n",
    "---\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d77b921-a72c-4f47-a708-915faf331fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_recipes.configs.datasets import alpaca_dataset\n",
    "from llama_recipes.utils.dataset_utils import get_dataloader\n",
    "        \n",
    "\n",
    "# Read the .jsonl file and store data in a list\n",
    "\n",
    "\n",
    "alpaca_dataset.data_path = \"AlpacaTrainingData.json\"\n",
    "\n",
    "train_dataloader = get_dataloader(tokenizer, alpaca_dataset, train_config)\n",
    "eval_dataloader = get_dataloader(tokenizer, alpaca_dataset, train_config, \"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43546bd-bdf1-4b55-80e0-506b3f151325",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, prepare_model_for_kbit_training, LoraConfig\n",
    "from dataclasses import asdict\n",
    "from llama_recipes.configs import lora_config as LORA_CONFIG\n",
    "\n",
    "lora_config = LORA_CONFIG()\n",
    "lora_config.r = 8\n",
    "lora_config.lora_alpha = 32\n",
    "lora_dropout: float=0.01\n",
    "\n",
    "peft_config = LoraConfig(**asdict(lora_config))\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4311176b-ecf6-4fac-bdbc-ba39b4602c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from llama_recipes.utils.train_utils import train\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "model.train()\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=train_config.lr,\n",
    "            weight_decay=train_config.weight_decay,\n",
    "        )\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=train_config.gamma)\n",
    "\n",
    "# Start the training process\n",
    "results = train(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    eval_dataloader,\n",
    "    tokenizer,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    train_config.gradient_accumulation_steps,\n",
    "    train_config,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    wandb_run=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28d0cec-a7f9-4077-aff8-90b1431828a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(train_config.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f773ccf3-2ce0-4bea-b29e-b94fce37af94",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "new_model_input = tokenizer(\"Hey mom, how are you today?\", return_tensors=\"pt\").to(\"cuda\")\n",
    "with torch.inference_mode():\n",
    "    print(tokenizer.decode(model.generate(**new_model_input, max_new_tokens=100)[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ababbe53-f46d-4ce9-8e28-a6e82955caca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_env)",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
