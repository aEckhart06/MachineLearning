{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6842509e-d7c6-46fe-a794-401a88a15e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD THE MODEL\n",
    "\n",
    "import torch\n",
    "from transformers import LlamaForCausalLM, AutoTokenizer\n",
    "from llama_recipes.configs import train_config as TRAIN_CONFIG\n",
    "\n",
    "train_config = TRAIN_CONFIG()\n",
    "train_config.model_name = \"meta-llama/Meta-Llama-3.2-3B-Instruct\"\n",
    "train_config.num_epochs = 3\n",
    "train_config.run_validation = False\n",
    "train_config.gradient_accumulation_steps = 4\n",
    "train_config.batch_size_training = 4\n",
    "train_config.lr = 3e-4\n",
    "train_config.use_fast_kernels = True\n",
    "train_config.use_fp16 = True\n",
    "train_config.context_length = 1024 if torch.cuda.get_device_properties(0).total_memory < 16e9 else 2048 # T4 16GB or A10 24GB\n",
    "train_config.batching_strategy = \"packing\"\n",
    "train_config.output_dir = \"meta-llama-mom\"\n",
    "train_config.use_peft = True\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    ")\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "            train_config.model_name,\n",
    "            device_map=\"auto\",\n",
    "            quantization_config=config,\n",
    "            use_cache=False,\n",
    "            attn_implementation=\"sdpa\" if train_config.use_fast_kernels else None,\n",
    "            torch_dtype=torch.float16,\n",
    "        )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(train_config.model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cb8e77-9705-4cf1-9626-d7c171d1c051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK BASE MODEL\n",
    "\n",
    "eval_prompt = \"\"\"\n",
    "Summarize this dialog:\n",
    "A: Hi Tom, are you busy tomorrow’s afternoon?\n",
    "B: I’m pretty sure I am. What’s up?\n",
    "A: Can you go with me to the animal shelter?.\n",
    "B: What do you want to do?\n",
    "A: I want to get a puppy for my son.\n",
    "B: That will make him so happy.\n",
    "A: Yeah, we’ve discussed it many times. I think he’s ready now.\n",
    "B: That’s good. Raising a dog is a tough issue. Like having a baby ;-) \n",
    "A: I'll get him one of those little dogs.\n",
    "B: One that won't grow up too big;-)\n",
    "A: And eat too much;-))\n",
    "B: Do you know which one he would like?\n",
    "A: Oh, yes, I took him there last Monday. He showed me one that he really liked.\n",
    "B: I bet you had to drag him away.\n",
    "A: He wanted to take it home right away ;-).\n",
    "B: I wonder what he'll name it.\n",
    "A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))\n",
    "---\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aadbeab-6a15-4f46-9683-9f39ef954bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD PREPROCESSED DATASET\n",
    "from llama_recipes.utils.dataset_utils import get_dataloader\n",
    "\n",
    "data = []\n",
    "with open('/Users/drew/Coding_Projects/HeyMomFirstDraft/TrainingData.jsonl', 'r') as file:\n",
    "    for line in file:\n",
    "        data.append(json.loads(line.strip()))\n",
    " \n",
    "# Split the data into training and validation sets\n",
    "\n",
    "train_size = int(0.8 * len(data))  # 80% for training\n",
    "train_data = data[:train_size]\n",
    "val_data = data[train_size:]\n",
    "\n",
    "\n",
    "# Convert to Hugging Face Datasets\n",
    "#dataset = Dataset.from_dict({key: [d[key] for d in data] for key in data[0]})\n",
    "\n",
    "train_dataset = Dataset.from_dict({key: [d[key] for d in train_data] for key in train_data[0]})\n",
    "val_dataset = Dataset.from_dict({key: [d[key] for d in val_data] for key in val_data[0]})\n",
    "print(train_dataset)\n",
    "print(val_dataset)\n",
    "\n",
    "\n",
    "train_dataloader = get_dataloader(tokenizer, train_dataset, train_config)\n",
    "eval_dataloader = get_dataloader(tokenizer, val_dataset, train_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7bde9e-133b-45be-b9a8-5714ce93e486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARE MODEL FOR PEFT\n",
    "\n",
    "from peft import get_peft_model, prepare_model_for_kbit_training, LoraConfig\n",
    "from dataclasses import asdict\n",
    "from llama_recipes.configs import lora_config as LORA_CONFIG\n",
    "\n",
    "lora_config = LORA_CONFIG()\n",
    "lora_config.r = 8\n",
    "lora_config.lora_alpha = 32\n",
    "lora_dropout: float=0.01\n",
    "\n",
    "peft_config = LoraConfig(**asdict(lora_config))\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a86e9d-9892-4048-b6e2-161eea506a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINE-TUNE THE MODEL\n",
    "\n",
    "import torch.optim as optim\n",
    "from llama_recipes.utils.train_utils import train\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "model.train()\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=train_config.lr,\n",
    "            weight_decay=train_config.weight_decay,\n",
    "        )\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=train_config.gamma)\n",
    "\n",
    "# Start the training process\n",
    "results = train(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    eval_dataloader,\n",
    "    tokenizer,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    train_config.gradient_accumulation_steps,\n",
    "    train_config,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    wandb_run=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc41828-9a6b-457b-a133-0c54618632e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE THE FINE-TUNED MODEL\n",
    "\n",
    "model.save_pretrained(train_config.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00d7a6e-6510-4f13-b127-6bf17c6105b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK OUT NEW MODEL\n",
    "new_model_input = \"Hey mom, how are you doing?\"\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    print(tokenizer.decode(model.generate(**new_model_input, max_new_tokens=100)[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_env)",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
